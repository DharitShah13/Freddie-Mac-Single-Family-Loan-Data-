{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT IF ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from webbot import Browser\n",
    "from zipfile import ZipFile\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "global scaler\n",
    "scaler = MinMaxScaler()\n",
    "from sklearn.metrics import *\n",
    "from IPython.core.display import HTML\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import datetime\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate(user='tiwari.abhi@husky.neu.edu', password=\"\\nWIB[Lw\"):\n",
    "    global web\n",
    "    web = Browser()\n",
    "    web.go_to(\"https://freddiemac.embs.com/FLoan/secure/login.php?pagename=download2\")\n",
    "    web.type(user, into='email')\n",
    "    web.type(password, into='password')\n",
    "    web.click('Submit Credentials')\n",
    "    web.click('Yes')\n",
    "    web.click('Continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_quarter_str(quarter_str='Q22005'):\n",
    "    try:\n",
    "        quarter_int = int(quarter_str[1:2])\n",
    "        \n",
    "        if quarter_int <= 0:\n",
    "            print('Quarter cannot be lesser than or equal to zero')\n",
    "            exit(0)\n",
    "        \n",
    "        year = int(quarter_str[2:])\n",
    "        \n",
    "    except:\n",
    "        print('Cannot Parse Quarter/Year to Int Value')\n",
    "        exit(0)\n",
    "    \n",
    "    next_quarter = quarter_int + 1\n",
    "    \n",
    "    if next_quarter > 4:\n",
    "        next_quarter = 1\n",
    "        year = year + 1\n",
    "    \n",
    "    next_quarter_str = 'Q'+ str(next_quarter) + str(year)\n",
    "    return next_quarter_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_4_quarter_data(first_quarter,second_quarter,third_quarter,fourth_quarter):\n",
    "    \n",
    "    quarters = [first_quarter,second_quarter,third_quarter,fourth_quarter]\n",
    "    \n",
    "    global file_names\n",
    "    file_names = []\n",
    "    \n",
    "    for q in quarters:\n",
    "        web.click('historical_data1_{}.zip'.format(q))\n",
    "        file_names.append('historical_data1_{}.zip'.format(q))\n",
    "        time.sleep(20)\n",
    "    \n",
    "    global download_path\n",
    "    download_path = ''\n",
    "    path_list = os.getcwd().split('\\\\')[:3]\n",
    "\n",
    "    for item in path_list:\n",
    "        download_path = download_path + item + '\\\\'\n",
    "\n",
    "    download_path = download_path + 'Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assure_path_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zipped_files_to_cwd(path_to_downloaded_files):\n",
    "    global folder_path_all_files\n",
    "    folder_path_all_files = os.getcwd() + '\\\\' + 'Extracted Quarterly Files'\n",
    "    zip_ref = zipfile.ZipFile(path_to_downloaded_files, 'r')\n",
    "    assure_path_exists(folder_path_all_files)\n",
    "    zip_ref.extractall(folder_path_all_files)\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNAN_orig(df):\n",
    "    df['fico'] = df['fico'].fillna(0)\n",
    "    df['flag_fthb'] = df['flag_fthb'].fillna('X')\n",
    "    df['cd_msa'] = df['cd_msa'].fillna(0)\n",
    "    df['mi_pct'] = df['mi_pct'].fillna(0)\n",
    "    df['cnt_units'] = df['cnt_units'].fillna(0)\n",
    "    df['occpy_sts'] = df['occpy_sts'].fillna('X')\n",
    "    df['cltv'] = df['cltv'].fillna(0)\n",
    "    df['dti'] = df['dti'].fillna(0)\n",
    "    df['ltv'] = df['ltv'].fillna(0)\n",
    "    df['channel'] = df['channel'].fillna('X')\n",
    "    df['ppmt_pnlty'] = df['ppmt_pnlty'].fillna('X')\n",
    "    df['prop_type'] = df['prop_type'].fillna('XX')\n",
    "    df['zipcode'] = df['zipcode'].fillna(0)\n",
    "    df['loan_purpose'] = df['loan_purpose'].fillna('X')\n",
    "    df['cnt_borr'] = df['cnt_borr'].fillna(0)\n",
    "    df['flag_sc'] = df['flag_sc'].fillna('N')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changedatatype_orig(df):\n",
    "    # Change the data types for all column\n",
    "    df[['fico', 'cd_msa', 'mi_pct', 'cnt_borr', 'cnt_units', 'cltv', 'dti', 'orig_upb', 'ltv', 'zipcode',\n",
    "        'orig_loan_term']] = df[['fico', 'cd_msa', 'mi_pct', 'cnt_borr', 'cnt_units', 'cltv', 'dti', 'orig_upb', 'ltv', 'zipcode','orig_loan_term']].astype('int64')\n",
    "    df[['flag_sc', 'servicer_name']] = df[['flag_sc', 'servicer_name']].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Origination_Quarter_File(historic_orig_files):\n",
    "    writeHeader1 = True\n",
    "\n",
    "    abc = glob.glob(historic_orig_files)\n",
    "    xyz = glob.glob(folder_path_all_files + \"/historical_data1_time_*.txt\")\n",
    "    \n",
    "    #Get Rid of all the paths that point to the historical_data1_time_files\n",
    "    for item in xyz:\n",
    "        abc.remove(item)\n",
    "    \n",
    "    for f in abc:\n",
    "        sample_df = pd.read_csv(f, sep=\"|\",\n",
    "                                names=['fico', 'dt_first_pi', 'flag_fthb', 'dt_matr', 'cd_msa', \"mi_pct\",\n",
    "                                       'cnt_units', 'occpy_sts', 'cltv', 'dti', 'orig_upb', 'ltv', 'int_rt',\n",
    "                                       'channel', 'ppmt_pnlty', 'prod_type', 'st', 'prop_type', 'zipcode',\n",
    "                                       'id_loan', 'loan_purpose', 'orig_loan_term', 'cnt_borr', 'seller_name',\n",
    "                                       'servicer_name', 'flag_sc'], skipinitialspace=True, low_memory=False)\n",
    "        \n",
    "        sample_df = fillNAN_orig(sample_df)\n",
    "        sample_df = changedatatype_orig(sample_df)\n",
    "        \n",
    "        sample_df['Year'] = ['19' + x if x == '99' else '20' + x for x in (sample_df['id_loan'].apply(lambda x: x[2:4]))]\n",
    "        \n",
    "        #Get historical File name\n",
    "        file_name = f.split('\\\\')[-1].split('.')[0]\n",
    "        \n",
    "        sample_df.to_csv(file_name + '.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAllReqColumns(df):\n",
    "    cols_to_keep = ['fico','flag_fthbN','flag_fthbX','flag_fthbY','mi_pct','cnt_units','occpy_stsl','occpy_stsO','occpy_stsS','cltv','dti','orig_upb','ltv','int_rt','channelB','channelC','channelR','channelT','ppmt_pnltyN','ppmt_pnltyX','ppmt_pnltyY','prop_typeCO','prop_typeCP','prop_typeLH','prop_typeMH','prop_typePU','prop_typeSF','prop_typeXX','loan_purposeC','loan_purposeN','loan_purposeP','orig_loan_term','cnt_borr']\n",
    "    \n",
    "    for x in cols_to_keep:\n",
    "        if not x in df.columns:\n",
    "            df[x] = 0.0\n",
    "            \n",
    "    df = df._get_numeric_data()\n",
    "    df.drop('cd_msa',axis=1,inplace=True)\n",
    "    df.drop('dt_first_pi',axis=1,inplace=True)\n",
    "    df.drop('dt_matr',axis=1,inplace=True)\n",
    "    df.drop('flag_sc',axis=1,inplace=True)\n",
    "    df.drop('zipcode',axis=1,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(df):\n",
    "    dummies1 = pd.get_dummies(df['flag_fthb']).rename(columns=lambda x: 'flag_fthb' + str(x))\n",
    "    train_df = pd.concat([df, dummies1], axis=1)\n",
    "    \n",
    "    dummies2 = pd.get_dummies(df['occpy_sts']).rename(columns=lambda x: 'occpy_sts' + str(x)) \n",
    "    train_df = pd.concat([train_df, dummies2], axis=1)\n",
    "    \n",
    "    dummies3 = pd.get_dummies(df['channel']).rename(columns=lambda x: 'channel' + str(x)) \n",
    "    train_df = pd.concat([train_df, dummies3], axis=1)\n",
    "    \n",
    "    dummies4 = pd.get_dummies(df['ppmt_pnlty']).rename(columns=lambda x: 'ppmt_pnlty' + str(x)) \n",
    "    train_df = pd.concat([train_df, dummies4], axis=1)\n",
    "    \n",
    "    dummies5 = pd.get_dummies(df['prop_type']).rename(columns=lambda x: 'prop_type' + str(x)) \n",
    "    train_df = pd.concat([train_df, dummies5], axis=1)\n",
    "    \n",
    "    dummies6 = pd.get_dummies(df['loan_purpose']).rename(columns=lambda x: 'loan_purpose' + str(x)) \n",
    "    train_df = pd.concat([train_df, dummies6], axis=1)\n",
    "    \n",
    "    train_df['flag_sc'] = train_df['flag_sc'].map({'Y':1,'N':0})\n",
    "    \n",
    "    train_df = checkAllReqColumns(train_df)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model,predicted_val,true_val):\n",
    "    RSq = r2_score(true_val,predicted_val)\n",
    "    print('R Squared: ' + str(RSq))\n",
    "    MAE = mean_absolute_error(true_val,predicted_val)\n",
    "    print('MAE: ' + str(MAE))\n",
    "    RMS = np.sqrt(mean_squared_error(true_val,predicted_val))\n",
    "    print('RMS: ' + str(RMS))\n",
    "    MAPE = np.mean(np.abs((true_val - predicted_val) / true_val)) * 100\n",
    "    print('MAPE: ' + str(MAPE))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Crisis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter the first QuarterYear as QXYYYY: Q12005\n"
     ]
    }
   ],
   "source": [
    "first_qtr_str = input('Please Enter the first QuarterYear as QXYYYY: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_qtr_str = get_next_quarter_str(first_qtr_str)\n",
    "if int(second_qtr_str[2:]) not in list(range(2005,2017)):\n",
    "        print(\"The Quarters requested are out of range\")\n",
    "        exit(0)\n",
    "        \n",
    "third_qtr_str = get_next_quarter_str(second_qtr_str)\n",
    "if int(third_qtr_str[2:]) not in list(range(2005,2017)):\n",
    "        print(\"The Quarters requested are out of range\")\n",
    "        exit(0)\n",
    "        \n",
    "fourth_qtr_str = get_next_quarter_str(third_qtr_str)\n",
    "if int(fourth_qtr_str[2:]) not in list(range(2005,2017)):\n",
    "        print(\"The Quarters requested are out of range\")\n",
    "        exit(0)\n",
    "\n",
    "navigate()\n",
    "get_4_quarter_data(first_qtr_str,second_qtr_str,third_qtr_str,fourth_qtr_str)\n",
    "\n",
    "for file in file_names:\n",
    "    extract_zipped_files_to_cwd(download_path + '\\\\' + file)\n",
    "\n",
    "get_Origination_Quarter_File(folder_path_all_files + \"/historical_data1_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features Obtained from FWD and BWD Search\n",
    "list_of_features = ['fico','mi_pct','cnt_units','orig_upb','ltv','orig_loan_term','cnt_borr','Year','flag_fthbN',\n",
    "                   'flag_fthbY','occpy_stsI','occpy_stsP','occpy_stsS','channelT','ppmt_pnltyN','prop_typeCO',\n",
    "                   'prop_typeCP','prop_typeMH','int_rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(file_names[0].split('.')[0] + '.csv',low_memory=False)\n",
    "df2 = pd.read_csv(file_names[1].split('.')[0] + '.csv',low_memory=False)\n",
    "df3 = pd.read_csv(file_names[2].split('.')[0] + '.csv',low_memory=False)\n",
    "df4 = pd.read_csv(file_names[3].split('.')[0] + '.csv',low_memory=False)\n",
    "\n",
    "processed_df1 = prepare_data_for_model(df1)\n",
    "processed_df2 = prepare_data_for_model(df2)\n",
    "processed_df3 = prepare_data_for_model(df3)\n",
    "processed_df4 = prepare_data_for_model(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = LinearSVR(C=0.01, dual=True, epsilon=0.01, loss=\"squared_epsilon_insensitive\", tol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=0.01, dual=True, epsilon=0.01, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='squared_epsilon_insensitive',\n",
       "     max_iter=1000, random_state=None, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.fit(processed_df1[list_of_features].drop('int_rt',axis=1),processed_df1['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: -0.829132932203128\n",
      "MAE: 0.3925374409746453\n",
      "RMS: 0.47415649069235977\n",
      "MAPE: 6.954675205285368\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted1 = svr.predict(processed_df2[list_of_features].drop('int_rt',axis=1))\n",
    "compute_metrics(svr,predicted1,processed_df2['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=0.01, dual=True, epsilon=0.01, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='squared_epsilon_insensitive',\n",
       "     max_iter=1000, random_state=None, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.fit(processed_df2[list_of_features].drop('int_rt',axis=1),processed_df2['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: -9.541401518422159\n",
      "MAE: 0.9284391769860116\n",
      "RMS: 1.1072312131794202\n",
      "MAPE: 16.107478137017644\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted2 = svr.predict(processed_df3[list_of_features].drop('int_rt',axis=1))\n",
    "compute_metrics(svr,predicted2,processed_df3['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=0.01, dual=True, epsilon=0.01, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='squared_epsilon_insensitive',\n",
       "     max_iter=1000, random_state=None, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.fit(processed_df3[list_of_features].drop('int_rt',axis=1),processed_df3['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: -0.168102644176632\n",
      "MAE: 0.3304790321780425\n",
      "RMS: 0.4183467283827056\n",
      "MAPE: 5.571970644563481\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted3 = svr.predict(processed_df4[list_of_features].drop('int_rt',axis=1))\n",
    "compute_metrics(svr,predicted3,processed_df4['int_rt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is an increase in the Errors as we predict the next quarter interest rate from the rolling data of the previous years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Years Later :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_qtr_str = 'Q12009'\n",
    "second_qtr_str = get_next_quarter_str(first_qtr_str)\n",
    "if int(second_qtr_str[2:]) not in list(range(2005,2017)):\n",
    "        print(\"The Quarters requested are out of range\")\n",
    "        exit(0)\n",
    "        \n",
    "third_qtr_str = get_next_quarter_str(second_qtr_str)\n",
    "if int(third_qtr_str[2:]) not in list(range(2005,2017)):\n",
    "        print(\"The Quarters requested are out of range\")\n",
    "        exit(0)\n",
    "        \n",
    "fourth_qtr_str = get_next_quarter_str(third_qtr_str)\n",
    "if int(fourth_qtr_str[2:]) not in list(range(2005,2017)):\n",
    "        print(\"The Quarters requested are out of range\")\n",
    "        exit(0)\n",
    "\n",
    "navigate()\n",
    "get_4_quarter_data(first_qtr_str,second_qtr_str,third_qtr_str,fourth_qtr_str)\n",
    "\n",
    "for file in file_names:\n",
    "    extract_zipped_files_to_cwd(download_path + '\\\\' + file)\n",
    "\n",
    "get_Origination_Quarter_File(folder_path_all_files + \"/historical_data1_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(file_names[0].split('.')[0] + '.csv',low_memory=False)\n",
    "df2 = pd.read_csv(file_names[1].split('.')[0] + '.csv',low_memory=False)\n",
    "df3 = pd.read_csv(file_names[2].split('.')[0] + '.csv',low_memory=False)\n",
    "df4 = pd.read_csv(file_names[3].split('.')[0] + '.csv',low_memory=False)\n",
    "\n",
    "\n",
    "processed_df1 = prepare_data_for_model(df1)\n",
    "processed_df2 = prepare_data_for_model(df2)\n",
    "processed_df3 = prepare_data_for_model(df3)\n",
    "processed_df4 = prepare_data_for_model(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = LinearSVR(C=0.01, dual=True, epsilon=0.01, loss=\"squared_epsilon_insensitive\", tol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=0.01, dual=True, epsilon=0.01, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='squared_epsilon_insensitive',\n",
       "     max_iter=1000, random_state=None, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.fit(processed_df1[list_of_features].drop('int_rt',axis=1),processed_df1['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: -0.3559264812191798\n",
      "MAE: 0.3372989255605931\n",
      "RMS: 0.4006004665145859\n",
      "MAPE: 7.09576531125427\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted1 = svr.predict(processed_df2[list_of_features].drop('int_rt',axis=1))\n",
    "compute_metrics(svr,predicted1,processed_df2['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=0.01, dual=True, epsilon=0.01, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='squared_epsilon_insensitive',\n",
       "     max_iter=1000, random_state=None, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.fit(processed_df2[list_of_features].drop('int_rt',axis=1),processed_df2['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: -0.8523449627617261\n",
      "MAE: 0.4505774209204374\n",
      "RMS: 0.5615577238188115\n",
      "MAPE: 8.511243447561862\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted2 = svr.predict(processed_df3[list_of_features].drop('int_rt',axis=1))\n",
    "compute_metrics(svr,predicted2,processed_df3['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=0.01, dual=True, epsilon=0.01, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='squared_epsilon_insensitive',\n",
       "     max_iter=1000, random_state=None, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.fit(processed_df3[list_of_features].drop('int_rt',axis=1),processed_df3['int_rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: -1.1269537027199874\n",
      "MAE: 0.38420444523483277\n",
      "RMS: 0.5289224513057039\n",
      "MAPE: 7.71183360420543\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted3 = svr.predict(processed_df4[list_of_features].drop('int_rt',axis=1))\n",
    "compute_metrics(svr,predicted3,processed_df4['int_rt'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
